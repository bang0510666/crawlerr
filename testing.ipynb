{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c40da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標題: [新聞] 志願役招募不順 立委馬文君提議：當兵加\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541358.A.B09.html\n",
      "標題: [問卦] 館長：我的便當在林口沒對手\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541451.A.C07.html\n",
      "標題: [問卦] 咖啡杯多久洗一次\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541484.A.826.html\n",
      "標題: Re: [新聞] 沖繩知事反對「台灣有事即日本有事」 外\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541502.A.2FF.html\n",
      "標題: Re: [新聞] 白宮「不明粉末」竟是古柯鹼！　當局展開\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541598.A.A1B.html\n",
      "標題: [問卦] 統一發票中獎可以去超商換錢嗎??\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541634.A.6DE.html\n",
      "標題: [問卦] 環球影城PASS門票一張五千真的有人會買?\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541637.A.DCA.html\n",
      "標題: Re: [爆卦] 郭董確定參加716遊行\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541682.A.24D.html\n",
      "標題: [問卦] 比有錢台灣人還是贏中國人啊\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541691.A.91A.html\n",
      "標題: [問卦] 機車yter是不是被車行跟油商綁架了?\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688541728.A.1BD.html\n",
      "標題: [公告] 八卦板板規(2023.03.01)\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1677600392.A.D12.html\n",
      "標題: [協尋] 6/28 國1北苗栗段 早上車禍 行車記錄器\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1687933173.A.900.html\n",
      "標題: [協尋] 6/30 16:30國1北上33.8k車禍 行車紀錄\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688143252.A.28E.html\n",
      "標題: [協尋] 6/28晚 重新路3段 行車記錄器（按推P幣）\n",
      "連結: https://www.ptt.cc/bbs/Gossiping/M.1688299105.A.453.html\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def scrape_articles(board, start_date=None, end_date=None):\n",
    "    # 設定Chrome Driver的執行檔路徑\n",
    "    options = Options()\n",
    "    options.chrome_executable_path = \"C:/Users/05731/OneDrive/桌面/crawler/chromedriver.exe\"\n",
    "\n",
    "    # 建立Driver物件實體，用程式操作瀏覽器運作\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # 設定起始頁面URL\n",
    "    base_url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # 等待同意條款視窗出現\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//button[@name='yes']\")))\n",
    "\n",
    "    # 點擊同意條款連結\n",
    "    agree_button = driver.find_element(By.XPATH, \"//button[@name='yes']\")\n",
    "    agree_button.click()\n",
    "\n",
    "    # 等待文章元素加載\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, \"title\")))\n",
    "\n",
    "    # 取得當前日期\n",
    "    current_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # 設定CSV檔案名稱\n",
    "    filename = f\"{board}_articles_{current_date}.csv\"\n",
    "\n",
    "    # 建立CSV檔案並寫入標題行\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"標題\", \"發文時間\", \"作者\", \"內容\", \"留言\"])  # 加入留言欄位\n",
    "\n",
    "        # 爬取文章\n",
    "        page_url = base_url\n",
    "        while True:\n",
    "            # 取得文章元素列表\n",
    "            article_elements = driver.find_elements(By.CLASS_NAME, \"title\")\n",
    "\n",
    "            for article_element in article_elements:\n",
    "                # 取得文章標題\n",
    "                title = article_element.text.strip()\n",
    "\n",
    "                # 點擊文章連結進入文章頁面\n",
    "                title_link = article_element.find_element(By.CSS_SELECTOR, \"a\")\n",
    "                link = title_link.get_attribute(\"href\")\n",
    "                driver.execute_script(\"window.open(arguments[0]);\", link)\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "                # 等待文章頁面加載\n",
    "                WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"main-content\")))\n",
    "\n",
    "                print(\"標題:\", title)\n",
    "                print(\"連結:\", link)\n",
    "\n",
    "                # 取得文章發文時間、作者和內容\n",
    "                post_time_element = driver.find_element(By.CSS_SELECTOR, \"div.article-metaline:nth-child(4) span.article-meta-value\")\n",
    "                post_time = post_time_element.text.strip()\n",
    "                author_element = driver.find_element(By.CSS_SELECTOR, \"div.article-metaline:nth-child(1) span.article-meta-value\")\n",
    "                author = author_element.text.strip()\n",
    "\n",
    "                content_element = driver.find_element(By.ID, \"main-content\")\n",
    "                content = content_element.text.strip()\n",
    "\n",
    "                # 精簡內容\n",
    "                content = content.replace(author, \"\").replace(post_time, \"\").replace(title, \"\")\n",
    "                lines = content.strip().splitlines()\n",
    "                cleaned_content = \" \".join(line for line in lines if line.strip())\n",
    "                cleaned_content = re.sub(r'//.*', '', cleaned_content)\n",
    "\n",
    "                # 取得留言內容\n",
    "                comments_elements = driver.find_elements(By.CSS_SELECTOR, \"div.push span.push-content\")\n",
    "                comments = [comment.text.strip() for comment in comments_elements]\n",
    "\n",
    "                # 檢查是否有隱藏的留言\n",
    "                hidden_comments_elements = driver.find_elements(By.CSS_SELECTOR, \"div.push span.push-content.hidden\")\n",
    "                for hidden_element in hidden_comments_elements:\n",
    "                    driver.execute_script(\"arguments[0].style.display = 'block';\", hidden_element)\n",
    "                    comments.append(hidden_element.text.strip())\n",
    "\n",
    "                # 寫入CSV檔案\n",
    "                writer.writerow([title, post_time, author, cleaned_content, \"\\n\".join(comments)])  # 將留言內容串接成字串\n",
    "\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "            # 翻頁\n",
    "            previous_page = driver.find_element(By.LINK_TEXT, \"‹ 上頁\")\n",
    "            page_url = previous_page.get_attribute(\"href\")\n",
    "            driver.get(page_url)\n",
    "\n",
    "            current_date = page_url.split(\"/\")[-1].replace(\"index\", \"\").replace(\".html\", \"\")\n",
    "\n",
    "            if start_date and end_date:\n",
    "                if start_date > current_date or current_date > end_date:\n",
    "                    break\n",
    "\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.LINK_TEXT, \"‹ 上頁\")))\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# 指定看板名稱，起始日期和結束日期\n",
    "scrape_articles(\"Gossiping\", start_date=\"2023-07-04\", end_date=\"2023-07-05\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959256f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hotboards():\n",
    "    # 發送GET請求，獲取熱門看板頁面內容\n",
    "    response = requests.get(\"https://www.ptt.cc/bbs/hotboards.html\")\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # 取得看板列表\n",
    "    board_list = []\n",
    "    board_elements = soup.find_all(\"div\", class_=\"b-ent\")\n",
    "    for board_element in board_elements:\n",
    "        name_element = board_element.find(\"div\", class_=\"board-name\")\n",
    "        name = name_element.text.strip()\n",
    "        board_list.append(name)\n",
    "\n",
    "    # 設定CSV檔案名稱\n",
    "    current_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"hotboards_{current_date}.csv\"\n",
    "\n",
    "    # 寫入CSV檔案\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"看板名稱\"])  # 加入欄位名稱行\n",
    "        writer.writerows([[board] for board in board_list])\n",
    "\n",
    "def scrape_articles(board, start_date=None, end_date=None):\n",
    "    # 取得當前日期\n",
    "    current_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # 設定CSV檔案名稱\n",
    "    filename = f\"{board}_articles_{current_date}.csv\"\n",
    "\n",
    "    # 建立CSV檔案並寫入標題行\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"標題\", \"發文時間\", \"作者\", \"內容\"])  # 不包含留言欄位\n",
    "\n",
    "        # 爬取文章\n",
    "        page_url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "        while True:\n",
    "            # 發送GET請求，獲取看板頁面內容\n",
    "            response = requests.get(page_url)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # 取得文章列表\n",
    "            article_elements = soup.find_all(\"div\", class_=\"r-ent\")\n",
    "            for article_element in article_elements:\n",
    "                # 取得文章標題\n",
    "                title_element = article_element.find(\"div\", class_=\"title\")\n",
    "                title = title_element.text.strip()\n",
    "\n",
    "                # 取得文章發文時間、作者和內容\n",
    "                post_time_element = article_element.find(\"div\", class_=\"date\")\n",
    "                post_time = post_time_element.text.strip()\n",
    "\n",
    "                author_element = article_element.find(\"div\", class_=\"author\")\n",
    "                author = author_element.text.strip()\n",
    "\n",
    "                # 點擊文章連結進入文章頁面\n",
    "                link = article_element.find(\"a\")[\"href\"]\n",
    "                article_url = f\"https://www.ptt.cc{link}\"\n",
    "                response = requests.get(article_url)\n",
    "                article_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                content_element = article_soup.find(\"div\", id=\"main-content\")\n",
    "                content = content_element.text.strip()\n",
    "\n",
    "                # 精簡內容\n",
    "                content = re.sub(r\"※ 發信站:.*\", \"\", content)\n",
    "                content = re.sub(r\"※ 文章網址:.*\", \"\", content)\n",
    "                content = re.sub(r\"※ 編輯:.*\", \"\", content)\n",
    "\n",
    "                # 寫入CSV檔案\n",
    "                writer.writerow([title, post_time, author, content])\n",
    "\n",
    "            # 翻頁\n",
    "            previous_page = soup.find(\"a\", string=\"‹ 上頁\")\n",
    "            if previous_page is None:\n",
    "                break\n",
    "\n",
    "            page_url = f\"https://www.ptt.cc{previous_page['href']}\"\n",
    "\n",
    "            # 檢查日期範圍\n",
    "            current_date = page_url.split(\"/\")[-1].replace(\"index\", \"\").replace(\".html\", \"\")\n",
    "            if start_date and end_date:\n",
    "                if start_date > current_date or current_date > end_date:\n",
    "                    break\n",
    "\n",
    "def scrape_comments(board, start_date=None, end_date=None):\n",
    "    # 取得當前日期\n",
    "    current_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # 設定CSV檔案名稱\n",
    "    filename = f\"{board}_comments_{current_date}.csv\"\n",
    "\n",
    "    # 建立CSV檔案並寫入標題行\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"標題\", \"發文時間\", \"作者\", \"留言\"])  # 只包含留言欄位\n",
    "\n",
    "        # 爬取留言\n",
    "        page_url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "        while True:\n",
    "            # 發送GET請求，獲取看板頁面內容\n",
    "            response = requests.get(page_url)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # 取得文章列表\n",
    "            article_elements = soup.find_all(\"div\", class_=\"r-ent\")\n",
    "            for article_element in article_elements:\n",
    "                # 取得文章標題\n",
    "                title_element = article_element.find(\"div\", class_=\"title\")\n",
    "                title = title_element.text.strip()\n",
    "\n",
    "                # 取得文章發文時間、作者\n",
    "                post_time_element = article_element.find(\"div\", class_=\"date\")\n",
    "                post_time = post_time_element.text.strip()\n",
    "\n",
    "                author_element = article_element.find(\"div\", class_=\"author\")\n",
    "                author = author_element.text.strip()\n",
    "\n",
    "                # 點擊文章連結進入文章頁面\n",
    "                link = article_element.find(\"a\")[\"href\"]\n",
    "                article_url = f\"https://www.ptt.cc{link}\"\n",
    "                response = requests.get(article_url)\n",
    "                article_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # 取得留言列表\n",
    "                comment_elements = article_soup.find_all(\"div\", class_=\"push\")\n",
    "                for comment_element in comment_elements:\n",
    "                    comment = comment_element.find(\"span\", class_=\"push-content\").text.strip()\n",
    "                    writer.writerow([title, post_time, author, comment])\n",
    "\n",
    "            # 翻頁\n",
    "            previous_page = soup.find(\"a\", string=\"‹ 上頁\")\n",
    "            if previous_page is None:\n",
    "                break\n",
    "\n",
    "            page_url = f\"https://www.ptt.cc{previous_page['href']}\"\n",
    "\n",
    "            # 檢查日期範圍\n",
    "            current_date = page_url.split(\"/\")[-1].replace(\"index\", \"\").replace(\".html\", \"\")\n",
    "            if start_date and end_date:\n",
    "                if start_date > current_date or current_date > end_date:\n",
    "                    break\n",
    "\n",
    "# 執行爬蟲任務\n",
    "scrape_hotboards()\n",
    "scrape_articles(\"Gossiping\")\n",
    "scrape_comments(\"Gossiping\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bec357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a663c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3277417328.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git init\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0f05c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
